---
title: "Clinical prediction with localized modeling using similarity-based cohorts: A scoping review"
author:
  - name: Adi Cohen
    email: ac4569@my.nsu.edu
    affiliation: 1
  - name: Patti McCall-Junkin
    email: pattimccall.junkin@gmail.com
    affiliation: 2
  - name: Jason Cory Brunson
    email: jason.brunson@medicine.ufl.edu
    affiliation: 3
    correspondingauthor: true
address:
  - code: 1
    address: College of Osteopathic Medicine, Nova Southeastern University, 3200 S University Drive, Davie, FL 33328
  - code: 2
    address: Smathers Libraries Academic Research & Consulting Services, University at Florida, P. O. Box 117000, Gainesville, FL 32611-7000
  - code: 3
    address: Laboratory for Systems Medicine, University of Florida, P.O. Box 100225 JHMHC, Gainesville, FL 32610-0225
abstract: |
  Background:
  Applications of classical case-based reasoning (CBR) to clinical and health tasks have given rise to a family of techniques in which an (often conventional) model is fitted to a subset of labeled cases, selected for relevance or similarity to an unlabeled case to which the model is applied. These localized models, as we propose to call them, may perform as well as "black-box" models while retaining direct interpretations. Their adoption has been hindered by fragmented development and implementation challenges. Our goal in this scoping review was to describe the clinical and health applications of localized models to date and to derive a general framework for their design and evaluation.
  
  Methods:
  We searched four electronic bibliographic platforms (PubMed, Web of Science, Academic Search Premier, and MathSciNet) during 2021 July 19–22. We screened and reviewed entries based on four criteria intended to specify applications of localized models to tasks involving analysis of clinical or health data. Two authors divided title/abstract screening and collaboratively reviewed screened entries for inclusion. We also screened and reviewed a seed set that inspired the review and traced methodological citations from included entries. We discussed and summarized settings, tasks, and tools; identified and tabulated themes; and synthesized the methods into a general framework. We received no specific funding for this work.
  
  Results:
  Of 1,309 search results, 328 were reviewed. Of these, combined with 43 publications that seeded the review and 1 obtained by citation tracking, 25 were included in the review and methodological synthesis. The specificity of several search terms was poor, and inter-rater reliability was low. Entries were published from 1997 to 2021, with half of these since 2015. The most common tasks were prognosis and diagnosis, with only few recent studies focusing on care recommendation. The studies overwhelmingly focused on clinical data, occasionally also including laboratory and image-derived data. Only three studies used localized models for any task other than prediction, most recently to identify significant determinants behind recommendation outputs. Several studies commented on memory and runtime costs, and four proposed partial solutions to them. While most techniques were not replicated and did not cite predecessors, a general technique that specializes to most of those encountered was straightforward to describe.
  
  Conclusions:
  Localized models hold promise to improve the performance of models used in clinical decision support tools while maintaining interpretability, provided their computational limitations can be overcome. Our search undoubtedly failed to capture all studies that used methods of this kind, but those reviewed and synthesized exhibit consistent enough design to be reproduced by a general-purpose tool. We hope that our review and synthesis will assist the development of more efficient implementations suitable for integration into practice.
keywords: 
  - case-based reasoning
  - localized modeling
  - nearest neighbors
  - clinical decision support
  - scoping review
journal: "Artificial Intelligence In Medicine"
date: "`r format(Sys.Date(), format = '%Y %b %d')`"
classoption: preprint, 3p, authoryear
bibliography:
  - SR-bibliography.bib
  - SR-0-Found.bib
  - SR-2-Included.bib
  - SR-3-Tracked.bib
linenumbers: true
numbersections: true
# Use a CSL with `citation_package = "default"`
# csl: https://www.zotero.org/styles/elsevier-harvard
# csl: https://www.zotero.org/styles/artificial-intelligence-in-medicine
csl: artificial-intelligence-in-medicine.csl
output: 
  rticles::elsevier_article:
    keep_tex: true
    latex_engine: xelatex
    # citation_package: natbib
    citation_package: "default"
header-includes:
  \usepackage{lscape}
  \newcommand{\landscapebegin}{\begin{landscape}}
  \newcommand{\landscapeend}{\end{landscape}}

---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
Sys.sleep(10)
tab_synthesis <- readr::read_rds(here::here("data/synthesis.rds"))
composite_studies <- readr::read_rds(here::here("data/composite.rds"))
tab_eval_comp <- readr::read_rds(here::here("data/eval-comp.rds"))
citation_id <- readr::read_rds(here::here("data/citations.rds"))
```

# Introduction

Clinical decision support (CDS) systems have been a fixture of artificial intelligence (AI) research since inception. As machine learning (ML) emerged, clinical modeling tasks became some of the most common and celebrated applications. A technique core to clinical applications of both AI and ML is _similarity-matching_, which relies on a _patient similarity measure_[^psm] to extract a cohort of past or training cases used to inform the diagnosis, prognosis, or care of a new or test case. Similarity-matching powers the retrieval step of most case-based reasoning (CBR) systems and provides the neighborhoods from which nearest neighbor (NN) models derive predictions. Whereas NN predictions are generated automatically via averaging (regression) or voting (classification), traditional CBR tools return the retrieved cases to the user to inform human decision-making.

[^psm]: Similarity measures between more granular units of analysis, such as encounters and decision points, are often still referred to as patient similarity measures.

We discuss in this review a hybrid computational approach we call _localized modeling_ that borrows elements from both clinical application areas and appears to have been introduced independently several times. The approach fits ML models, including regularized families of generalized regression models (GLMs), to cohorts retrieved via similarity matching. Whereas CBR and NN prediction use patient similarity to obtain an ideally homogeneous subpopulation matched to the index case, in localized modeling one seeks a relevant yet heterogeneous subpopulation, each member of which deviates slightly from the index case. In principle, this allows interpretable models like GLMs to identify and measure the effects of determining factors that are specific to the index case.
The use of similarity cohorts distinguishes localized modeling from classical GLMs that still dominate analysis in clinical trials and comparative effectiveness research, as well as from many cutting-edge ML models that are fitted to whole populations, while the cohorts' heterogeneity and its utility to the model distinguish the approach from CBR.
In the reviewed studies, localized modeling shows some promise toward simultaneously achieving the performance improvements of ML and the interpretability of classical models.

<!--
In multivariate settings, because similarity decreases with difference along each factor, included cases that differ more with respect to some factors will tend to differ less with respect to others. Consequently, (1) the distribution of a factor in the subpopulation will tend to be centered near its value at the index case, and (2) the joint distribution of a pair of factors will tend to be sparse away from both center axes, i.e. when both factors differ from their values at the index case. This should produce localized models that are less responsive to deviation along multiple uncorrelated factors, so that the influence of each factor in the model reflects more its independent effect than its interactions with other factors.
-->

One memorable aspiration of clinical informatics was the "green button" that would, in response to a query, automatically retrieve patient data from an institution’s records with which to conduct on-demand retrospective studies for an individual patient @Longhurst2014. Localized modeling has recently been put forth as one answer to this appeal. Studies using localized models have been limited by computational cost—a naïve implementation involves fitting, optimizing, and evaluating a model on a separate population for each training case—and recent studies have described algorithms and implementations designed to reduce this cost. As greater access to health data, informatical knowledge, algorithmic efficiency, and computational power continue to spur innovation in CDS, we may expect localized modeling to become more accessible and efficient, its advantages better understood, and its disadvantages ameliorated.

The motivation for this scoping review is to describe the settings in and problems with which localized models have been tasked to date, and to synthesize existing methods into a general framework. Along the way, we attempt to reconcile terminology, summarize evaluation techniques, propose needed follow-up work, and recommend best practices.

## Related work

Clinical CBR emerged among rule-based approaches and other AI tools in the development of expert systems [@Aamodt1994]. Early implementations realized a general workflow described as "the four REs", later the "R4 cycle" [@Aamodt1994; @Begum2011]: Given a new case (or problem), the system retrieves one or more past cases (solved problems) from a corpus, reuses these to generate an understanding of (solution to) the new case, revises this understanding to better fit the new case (sometimes called "adaptation"), and retains the new case and its eventual understanding in the corpus to be retrieved and reused in future. In contrast to rule-based systems, which are variable-based and generally interpretable as a single rule applied to all new cases, case-based systems provide case-specific interpretations in the form of a number of more fully understood reference cases. While the similarity measure used in the retrieval step need not be changed as the corpus grows, proposed measures have been diverse, contested, and rarely systematically compared.

@Kolodner1992 distinguished two styles of CBR: problem-solving, which is more procedural and used when objectives are more clearly defined, and interpretative, which provides categorizations and justifications for possible solutions. A similar dichotomy is commonly used to distinguish the performance criterion for the usefulness of predictive models from the interpretability criterion for their usability. In these terms, localized models are highly procedural: The step of fitting a predictive model to a retrieved cohort can be understood as an automated adaptive strategy [@Begum2011], and the parameters that govern cohort retrieval can be tuned alongside model hyperparameters in a ML workflow. Accordingly, localized models have been primarily used for and evaluated on their ability to more accurately predict classes or outcomes. Nevertheless, as some more recent applications have begun to show, they can be used to draw inferences about the importance of different risk factors to specific individuals. **Distinguish between interpretable model components like effect estimates and informative but not directly interpretable importance measures as are commonly computed for 'black-box' models.** While the patient similarity measure used to retrieve each cohort may be complicated, the cohort itself can be directly inspected by the user. Provided the model family fitted to the cohorts is interpretable, the localized model as a whole inherits this property. Thus, localized models may embody both styles of CBR.

We refer the interested reader to several previous reviews of CBR in medicine [@Gierl1998; @Begum2011; @Choudhury2016], of measures of patient similarity [@Dai2020], and of uses of patient similarity in predictive models [@Welch2013; @Sharafoddini2017; @Parimbelli2018]. While the reviews of CBR focus on applications using health data, the patient similarity reviews encompass many additional types of data (various molecular -omics, genetic tests, medical images, laboratory tests, patient preferences, patient-reported outcomes, tracking devices, social media) and survey a much broader scope of models (exploratory analysis via dimension reduction and cluster analysis; risk evaluation and outcome prediction; clinical decision support and software tools). For the present review, we are interested in how similarity matching on patient-level health data is used to construct local cohorts for predictive modeling.

## Objectives

Our goals in this review are (1) to describe applications to date of similarity-based localized models using patient-level health data and (2) to provide a general framework for the design and evaluation of such localized models. Our selection of relevant papers from the search corpus will be based on the following inclusion/exclusion criteria:

* Pulls case-level data from a corpus of past cases with known response values\newline
(response may be outcome, diagnosis, subtype, etc.)
* Defines a numeric multivariate case similarity measure\newline
(include integer-valued measures)
* Uses the similarity measure to select cohorts for index cases from the corpus\newline
(for example, based on a training–testing partition)
* Fits statistical models to cohorts from which to draw inferences about index cases\newline
(outcome predictions count, survival estimates, risk factor effect estimates, model evaluation statistics, etc.)

We expected the general framework to boil down to three choices: A patient similarity measure, a cohort selection process, and a statistical model family. Such a general method would be straightforward, if laborious, to implement, and specializations of it would account for the majority of real-world applications. A general implementation based on this method would allow researchers to expedite every step of the analysis process, including selection, optimization, and evaluation, and enable sensitivity, robustness, and multiverse analyses that help identify the most consequential choices along the way.

# Methods

We describe the steps of our review in detail in the Appendix.
We also note deviations from plans and the reasons for them.

## PRISMA checklist

We include a PRISMA checklist as Supplemental Table 1 and a PRISMA abstract checklist as Supplemental Table 2.
Because we focus on methodologies rather than approaches, we deviate in some ways from PRISMA guidelines. In particular, many items of the checklist, involving bias assessment and quantitative synthesis, are intended for meta-analyses and, therefore, did not apply to this study.
Because we are not aware of any standard procedures for conducting reviews and syntheses of methodology, no protocol was prepared for this study.

## Search

We derived the eligibility criteria itemized in the Introduction from a seed set of previously read studies. Based on these criteria, we formulated search strings for five platforms: PubMed, Web of Science, Academic Search Premier, Google Scholar, and MathSciNet. We finalized the PubMed search first, then adapted it to the other platforms (Sections \ref{sec:appendix-search}).

Through each platform, we searched those databases included by default.
This means that we searched both MEDLINE and PubMed Central (PMC) through Pubmed (we did not exclude other databases, but our earliest included results post-date them) and that we searched the six indices of the Core Collection (the Science Citation Index Expanded, the Social Sciences Citation Index, the Arts & Humanities Citation Index, the Emerging Sources Citation Index, the Conference Proceedings Citation Index, and the Book Citation Index) as well as several regional databases through the Web of Science platform.

The structure and terms of our search strings were inspired in part by previous reviews adjacent to our topic of interest [@Sharafoddini2017; @Parimbelli2018]. We organized the PubMed search in disjunctive normal form (an OR of ANDs). Following the solidification of an outline, we expanded each term to include synonyms that are similar enough to be applicable to our search. We then evaluated the expanded search string using the PubMed Advanced Search platform. We initially included each term and their synonyms separately to evaluate what resulted. We pruned several terms that returned no results ("phrases not found") or to reduce the number of results. At the conclusions of this process for each individual term, we combined the search terms using disjunctive normal form.

We conducted all searches over 2021 July 19–22. We tentatively excluded results from Google Scholar because they were missing abstracts, and later agreed to discard these results due to the lack of reproducibility of the search. We organized the remaining results into a public Zotero collection with one subcollection for each database. We then imported the pooled results to Covidence, which merged some duplicate entries.

## Title/abstract screen

Within Covidence, we screened titles and abstracts for relevance. We decided on four eligibility criteria to expedite the screening process. These include articles written in English for readability, original studies to strictly include primary sources with new information/conclusions, medical or clinical related settings due to the applicability of our data to this field, and alternative use of the key terms within the search string, meaning that it was clear from the title and abstract that the entry was included due to the use of one or more search terms with a different meaning than ours. Each of two authors (AC and JCB) screened roughly half of the entries. They regularly reviewed each other’s decisions to improve consistency. In cases of uncertainty, an entry would be included.

## Full text review

We set out 4 criteria for full-text review to restrict to studies that used some form of localized modeling on health data:

* Pulls case-level data from a corpus of past cases with known classes or outcomes
* Defines a numeric multivariate case similarity measure
* Uses the similarity measure to select cohorts for index cases from the corpus
* Fits statistical models to cohorts to make inferences about index cases

Note that classical CBR satisfies the first and third criteria by definition and in most cases will satisfy the second.

In Covidence, two authors (AC and JCB) independently evaluated each manuscript for the four eligibility criteria. The first criterion that a manuscript failed was designated the reason for exclusion. An antecedent criterion was used to exclude manuscripts that did not report the results of original studies involving real-world experiments or empirical data, for example surveys of prior work and proposals of frameworks. In cases of disagreement between the authors on the reason for exclusion, the first criterion was adopted. The authors arrived at agreement on inclusion or exclusion through discussion. We calculated inter-rater reliability to evaluate our screening and review process.

During full text review, we decided to expand the conception of statistical models (fourth criterion): Rather than restricting to models that are fit and evaluated in separate steps, we chose to allow simple statistical summaries such as mean survival [@Mariuzzi1997] and survival curves [@Lowsky2013]. These approaches were novel to CBR and presage later developments, so were helpful to understanding the development of localized modeling.
However, this then admitted studies that applied conventional nearest neighbors prediction: Each retrieved cohort consisted of the $k$ most similar cases to the index case, and the response for the index case was predicted to be either the mean (continuous response) or the plurality (discrete response) of the responses of the cohort. A review of all studies that use nearest neighbors prediction, even in the clinical domain, would be impractical. Because our focus is on novel approaches that combine similarity-based retrieval and statistical modeling of retrieved cohorts, we chose to exclude those studies whose approach was equivalent to nearest neighbors prediction using a conventional similarity measure.

Finally, one author (JCB) applied the same review process to the seed set of 43 studies that inspired the review.

## Coding

Following the selection sources aligning with our review, we collected characteristics of included studies (Section \ref{sec:data}). The features included bibliographic fields (date of publication, journal, authors, title, keywords, DOI), study goals (objective, generalizable knowledge, evaluation, clinical/medical domain), data sets (data source, type of data, range of data, number of cases/incidences, number of predictors/features), and methodological choices (types of similarity measure, families of adaptation step/statistical model, method(s) compared against, evaluation/performance/comparison measures (& their values), name given to modeling approach). We used these data to detect and visualize trends amongst the included studies.

## Synthesis

Rather than an evidence synthesis characteristic of most systematic reviews, we here pursue a methodology synthesis to harmonize possibly independent research efforts that have converged on a common modeling technique. The goal will be to describe a unified framework for localized models that can be used to guide future study designs and implementations as well as more systematically evaluate variations on the theme and measure the dependence of results on modeling choices.

# Results

## Selection

Figure \ref{fig:prisma} depicts our identification of studies via databases and registers. Following the completion and input of each search string, there were a total of 1,422 sources within all of the platforms used. De-duplication resulted in 1,309 entries, which were added to the title/abstract screening for review. Of these, 328 entries met the screening criteria and were assessed through full text review. Of these, 44 fit the original criteria, and 19 were included as distinct from NN prediction.

```{r fig:prisma, fig.align='center', out.width="80%", echo=FALSE}
#| fig.cap =
#| "\\label{fig:prisma}
#| PRISMA-S flow chart.
#|  Lowercase letters refer to items obtained from
#|  the seed set (m),
#|  the structured search (n), and
#|  citation/reference tracking (s)."
knitr::include_graphics("../../fig/fig-prisma.pdf")
```

There were 60 disagreements over inclusion versus exclusion.
Inter-rater reliability was low, at 82% relative to a 72% probability of random agreement.

We then reviewed 43 studies comprising a seed set that inspired this review. After removing duplicates and screening for eligibility, we were left with 6 additional studies [@Park2006; @Lowsky2013; @Lee2015; @Ng2015; @Lee2017; @Wang2019], 1 of which was excluded from the synthesis for using nearest neighbors prediction. Finally, citations in reference lists from the 43 + 328 = 371 studies assessed for eligibility led us to identify 1 additional study that met our criteria [@Kasabov2010]. This left us with 19 + 1 + 5 = 25 studies included in the review and synthesis.

## Bibliographic and methodological properties

The 25 included studies were analyzed based on 20 characteristics (see Section \ref{sec:data}), and we report and describe some observations here (Table \ref{tab:synthesis}). The studies were published in a variety of journals, with some of greater frequency, though no single journal published more than 3. The journal _Evolving Systems_ published 2 of the included studies, _Artificial Intelligence in Medicine_ published 3, and _Hindawi - Journal of Healthcare Engineering_ published 3.

Another interesting pattern lay in the years of publication of the included studies (Figure \ref{fig:year}). While they stretch back to the late 1990s, they have become more common, which suggests that this is still an active, though not rapidly expanding, methodological approach. We note that, while CBR in health and medicine originated as early as 1990, these studies did emerge early on once CBR had established itself. So the idea has been "in the air" for as long as CBR has been in use.

```{r tab:synthesis, echo=FALSE, message=FALSE, warning=FALSE}
tab_synthesis |> 
  # combine citations of the same study
  left_join(citation_id, by = "Citation") |> 
  group_by(across(c(-Number, -Citation, -Date))) |> 
  summarize(Date = min(Date), Citation = str_c(Citation, collapse = "; ")) |> 
  ungroup() |> 
  arrange(Date) |> 
  select(Citation, Task, Aim, Source, Type, Cases, Features) |> 
  # format citations for Markdown-LaTeX
  # mutate(Citation = str_c("[@", Citation, "]")) |> 
  mutate(Citation = sapply(
    str_split(Citation, "; "),
    function(s) str_c("@", s, collapse = "; ")
  )) |> 
  # prevent numeric list columns from dominating the horizontal space
  mutate(across(
    c(-Citation, -Cases, -Features),
    ~ str_c(., "\\hspace{6em}")
  )) |> 
  knitr::kable(
    align = "lllllrr",
    caption = paste0(
      "\\label{tab:synthesis}",
      "Studies included in the method synthesis,",
      " arranged by the earliest the study is known to have been public."
    )
  )
```

```{r fig:year, fig.align='center', out.width="75%", echo=FALSE}
#| fig.cap =
#| "\\label{fig:year}
#| Number of publications in our sample each year."
knitr::include_graphics("../../fig/fig-years.png")
```

Another dominant characteristic of included studies is their broader remit. We categorized studies as "knowledge" or "practice" based on whether their aim was to produce generalizable knowledge or to improve practice. We classified 7 of the 25 studies as "knowledge", making the majority of studies from a "practice" standpoint. These studies aimed primarily to provide providers with tools to use in clinic or to improve outcomes for patients currently receiving care.

Lastly, we observed a commonality among 6 of the 25 included studies in having evaluated methods using leave-one-out cross-validation (LOOCV). The purpose of this measure is to estimate the overall performance of certain factors when used to make predictions, particularly utilized on smaller data sets, where models benefit greatly from larger training sets and additional model fitting is less costly.

In addition to the aim of its analysis, we coded several aspects of the design of each study, including the source and type of data and the clinical task the model performed (Table \ref{tab:synthesis}), as well as the specific methodology and terminology adopted (Table \ref{tab:composite}).
In the following subsections, we take a closer look at these design elements.

```{r tab:composite, echo=FALSE, message=FALSE, warning=FALSE}
composite_studies |> 
  transmute(
    # Citation = str_c("@", Citation),
    Citation = sapply(
      str_split(Citation, "; "),
      function(s) str_c("@", s, collapse = "; ")
    ),
    Elements = str_replace_all(Elements, ",", ";"),
    Terminology
  ) |> 
  mutate(across(
    everything(),
    # ~ str_replace_all(., "<U+001B>", "")
    textclean::replace_non_ascii
  )) |>
  # mutate(across(c(Citation), ~ str_c(., "\\hspace{6em}"))) |> 
  knitr::kable(
    align = "lllllrr",
    caption = paste0(
      "\\label{tab:composite}",
      "Methodological elements of studies included in the synthesis. "
    )
  )
```

## Application domains

While all included studies were reported in scientific and medical journals, the vast majority were oriented toward clinical practice rather than medical research. For example, a 2006 study specifically evaluated the usefulness of CBR-based explanations for the purpose of decision support [@Doyle2006a]. This study was part of a much larger literature on CBR systems and was included here despite relying on NN prediction because it used an unconventional voting scheme to generate recommendations. A more recent study took essentially the same focus with respect to a proposed clinical risk prediction model, which amounted to CBR with a novel weighting scheme on predictors informed by expert consensus [@Fang2021a]. In both cases a prototype implementation was deployed in an experimental setting for evaluation.

The most common clinical motivations were individualized detection or diagnosis, prognosis or outcome prediction, and treatment or care recommendation. The plurality focused on prognosis or outcome prediction, often using time-to-event analysis: @Mariuzzi1997 used CBR to predict survival time from several geometric properties of breast tumors. @Lowsky2013 used CBR with non-parametric survival models on registry data to predict patient–graft survival times following kidney transplantation. @Lee2015 and @Lee2017 used localized logistic regression and random forest modeling to predict 30-day mortality following discharge for ICU patients. @Vilhena2016a developed a CBR cycle around a clustering-informed similarity matching procedure and an artificial neural network–based classifier to identify thrombophilia patients at high risk of thrombotic episodes. And @Ma2020a took a similarity cohort–based approach to predicting length of stay for ICU patients. Also of note, from a public health perspective, @Xu2008a used CBR to predict rehabilitation time as well as disability risk for unemployed workers experiencing chronic pain.

Toward detection and diagnosis, @Wyns2004a proposed a hybrid neural net–CBR system to distinguish (with confidence bounds) arthritic versus control patients, based on several histological features. @Nicolas2014a used collaborative multilabel CBR to subtype melanoma patients based on confocal and dermoscopy images. @Wang2019 used localized models built from a multi-type additive similarity measure to distinguish type 2 diabetic versus control populations. Along the way, @Song2006c, @Kasabov2010, and @Verma2015a took an iterative model-building approach to several tasks: predicting glomerular filtration rate, a key indicator of renal function, from demographic and physiological variables; identifying patients with colon cancer using a large number of gene expression measurements; and identifying patients with type 2 diabetes based on demographic, physiological, and genetic variables.

While several studies emphasized the potential or actual value to decision-making of their methods and tools, only one incorporated treatment decisions into their approach: By taking "decision points" as their units of analysis, @Tang2021c and @Ng2021b built not classifiers or predictors but comparative effectiveness models into a localized framework, providing for the first time in our sample explicitly prescriptive rather than descriptive clinical decision support.

A partial exception to this focus was a 2014 study that also reported a decision support tool, in this case for early diagnosis of melanoma from clinical data and dermoscopy images [@Nicolas2014a]. While the stated objectives were analogous, specific emphasis was placed on the acquisition of new knowledge through the development of the tool, including the systematic generation of new data and creation of a clinical ontology. This study was included for its use of a collaborative classifier that drew from multiple modeling approaches. In keeping with this focus of the included literature, the stated objectives of the proposed methods were more often (or additionally) to predict outcomes or to recommend interventions than only to diagnose disease.

The mosaic plot in Figure \ref{fig:properties} summarizes the relation between data source, clinical task, and aim.

```{r fig:properties, fig.align='center', out.width="100%", echo=FALSE}
#| fig.cap =
#| "\\label{fig:properties}
#| Share of studies characterized by several design elements:
#|  data source (row),
#|  clinical task (fill color), and
#|  remit (opacity)."
knitr::include_graphics("../../fig/fig-properties.png")
```

## Rationales
\label{sec:rationales}

The included studies hypothesized, asserted, or assumed several benefits of localized modeling specific to clinical and medical settings and advantages over other modeling approaches (see Section \ref{sec:data}).
The most common was that the restriction to similar or relevant past cases would improve predictive performance for the index case [@Mariuzzi1997; @Liang2015a; @Ng2015; @Lee2017]. In particular, @Lee2015 hypothesized and confirmed that the value of each past case would be positively related to its similarity to the index case, an assumption built in to the weighting schemes of other approaches. @Lowsky2013 made a different case, that the fewer parametric assumptions and complications of a CBR-style model would allow for greater accuracy.
Additionally, several investigators asserted that the use of localized cohorts befit the clinical focus on the individual patient rather than the population, without reference to model performance [@Song2006c; @Xu2008a].

In an interesting contrast, @Tang2021c and @Ng2021b argued that their localized approach using the larger and more heterogeneous populations covered by EHR-derived data could better capture the messy and diverse lessons of everyday practice, as a counterpart to guidelines based on randomized controlled trials.
Their emphasis on the enabling role of EHRs to power methods well-suited to large, structured data repositories was shared by several others [@CampilloGimenez2012a; @Nicolas2014a; @Verma2015a; @Lee2017].
@Verma2015a and @Zhang2018a additionally pointed out that localized models are adaptable to noise in the data, variation in patterns of missingness, and (harkening to the last step of the R4 cycle) addition of new cases with known outcomes to the corpus. These properties, they said, tend to be more difficult for whole-population models to handle.

The other frequent advantage attributed to localized modeling was interpretability.
@Elter2007a and @Nicolas2014a emphasized the value of the detailed past cases, available to the user, on which predictions are based. This "self-explanation capability" made outputs more intelligible to physicians in the CDS setting.
@Ng2015 additional pointed out that localized GLMs yield localized effect estimates, which may help investigators identify individually relevant risk factors.

The remaining coded rationales were for augmentations or hybridizations of then-conventional CBR, most of which defended the use of other tools to improve performance via cohort selection [@CampilloGimenez2012a; @Nicolas2014a; @Vilhena2016a] or to make models and outputs more interpretable [@Lopez2011a; @Wang2019]. @Liang2015a argued for simultaneous optimization of cohort and feature selection with model parameterization; their TWNFI approach combines localization with regularization.

## Challenges

CBR can be understood as the opposite side of a trade-off with rule-based reasoning between model size and model complexity; @Doyle2006a describe their approach as "knowledge-light", in that "the cases do not contain explicit explanation structures; instead, explanation is achieved by comparison of the query case with retrieved cases". This means that the greatest performance and efficiency challenges in CBR have to do with the retrieval and revision phases in the R4 cycle. Several studies addressed these challenges: @Park2006, while excluded from the synthesis, were the earliest to propose that cohorts be bounded by a similarity threshold rather than by a number of cases, and this improved performance in their experiments. @CampilloGimenez2013 leveraged predictor weights obtained from logistic regression to inform the similarity calculation used in retrieval. @Lowsky2013 used non-parametric models to reduce the computational burden of revision (prediction). @Ma2020a proposed to improve efficiency along the entire R4 cycle, but in particular by only executing task-dependent steps in real time ("just-in-time learning"), and @Ng2021b and @Tang2021c partitioned multiple phases in the R4 cycle into offline and online components, only the latter of which would be performed in real time as new data are received.

Other studies addressed limitations of available tools. @Lopez2011a implemented a comprehensive CBR tool in response to the lack of general-purpose software, to enable coupling with other tools as well as to expedite development, experimentation, and uptake. Several other teams also set out to develop  more interoperable clinical support tools [@Elter2007a; @Liang2015a; @Ng2015; @Zhang2018a].

The frequency plot in Figure \ref{fig:methods} compares the rates at which several methodological elements are invoked in the sample.

```{r fig:methods, fig.align='center', out.width="100%", echo=FALSE}
#| fig.cap =
#| "\\label{fig:methods}
#| Frequency of recurring methodological elements across included studies."
knitr::include_graphics("../../fig/fig-methods.png")
```

In the Discussion, we discuss trends in the literature we reviewed, identify common limitations and gaps in understanding, and attempt to synthesize their methods into a general framework.

## Performance assessments
\label{sec:performance}

Most included studies quantitatively compared the predictive performance of their proposed method(s) to one or more comparators. What we took to be the signature results are collated in Table \ref{tab:performance} in the Appendix. Note that we exercised some judgment in classifying methods as proposals and comparators, as in some cases all methods were original but only some showcased main ideas.
It would be impractical to meta-analyze these numbers due to the great variety of settings, problems, data types, and choices involved.

However, we do observe one clear pattern: Of the proposed methods that most evidently outperformed their comparators (Kohonen + CBR [@Wyns2004a], TWNFI [@Song2006c, @Kasabov2010], GSA [@Liang2015a], CBR + rules (+ DML) [@Nicolas2014a], Gaussian process regression [@Zhang2018a], JITL-ELM [@Ma2020a]), all except the last are hybrids of localized modeling (in some cases CBR) with other techniques, often metric learning. Though @CampilloGimenez2012a and [@Ng2015] report exceptions, the pattern suggests the importance of the similarity measure to the retrieval step.
Meanwhile, when proposed approaches targeted cohort demarcation or choice of predictive model (statistical CBR [@Park2006]; individualized logistic regression, decision tree, and random forest [@Lee2015; @Lee2017]), they did not consistently outperform comparators.

## Identified needs
\label{sec:needs}

The study authors focused their recommendations and their own plans for future work mostly on technical improvements and evaluations [@Mariuzzi1997; @Yearwood1997a].
Urged improvements included full or partial automation of user-dependent steps such as predictor selection [@Mariuzzi1997; @Yearwood1997a], similarity learning [@Mariuzzi1997; @Wang2019], and parameter optimization [@Song2006c; @Lee2017]; extensions to new data structures [@Lopez2011a], data types [@Liang2015a; @Verma2015a; @Malykh2018a], and reasoning systems [@Nicolas2014a]; and the use of more advanced model components to improve accuracy or efficiency [@Lowsky2013; @Lee2015; @Liang2015a; @Zhang2018a].
Authors also urged validations and independent evaluations using larger or more comprehensive data sets [@Elter2007a; @Xu2008a; @Verma2015a; @Ng2015], using data aggregated from multiple health systems [@Lee2015; @Lee2017; @Tang2021c; @Ng2021b], and in other care settings or disease contexts [@Song2006c; @Zhang2018a; @Tang2021c; @Ng2021b].
Less common were calls to strengthen the connection between the methods and the users.
Some authors urged the incorporation of (human-derived) domain knowledge into the data or models [@Yearwood1997a; @Wang2019]. Others suggested new uses of their modeling approaches: to measure feature importance [@Wyns2004a], to make models more expressive [@Lee2015], and to combine information obtained both from global and from localized models [@Ng2015].
Most of this incremental work was indeed carried out in later investigations in our sample.

## Public availability
\label{sec:availability}

**Discuss public availability of code or implementations.**
**Methods, results, and inferential reproducibility [@Matarese2022].**

# Discussion

We draw several observations from our encoding of the included studies. We first focus on two driving questions: What challenges did investigators set out to solve? How did their approaches meet these challenges? We then comment on the connectedness of the included studies and the potential for recombination.

## Process

The low inter-rater reliability was due, in parts, to different interpretations of some eligibility criteria by the authors, inconsistent terminology across the sample, and incomplete reporting of resources and methods in the sample. Regarding interpretation of criteria, some wording of the criteria was adjusted following discussions between the reviewing authors during full-text review to better specify an agreed-upon meaning. We will discuss the different domains, terminologies, and reporting issues of the sample in the remainder of this section.

## Study designs

Consistent with their orientation, almost all included studies were conducted using clinical data, only occasionally together with patient-reported (2), laboratory (2), and image (1) data. We suggest three reasons for this: First, this literature traces back to the 1990s, before -omic data could be generated cost-effectively at scale. Second, CBR in particular has a strong tradition in clinical decision support, where the focus of our sample remains throughout the review period. Third, because most -omic data are highly homogeneous—all measurements are made along or can be transformed to a common scale, e.g. greyscale pixellations for X-ray images and transcripts per million for RNA-seq data—more deeply theoretical analysis techniques have been developed and come into wide use. While variations on correlation-based approaches like EHR-based phenome-wide association studies and similarity-based methods like CBR itself have been developed, more mechanistic and probabilistic tools have not become a domain standard.

Because we excluded studies that used conventional NN prediction, many included studies reported new approaches to adaptation subsequent to retrieval. Very few proposed novel similarity measures, possibly because larger studies tended to be reported in separate articles detailing experiments with specific components of the process. However, this also suggests that few experimental studies have focused on the unified development of new retrieval and adaptation strategies.
We also note that the majority of studies evaluated and compared methods using leave-one-out cross-validation (LOOCV). This is an especially appropriate technique when available data are scarce, and indeed most data sets used by included studies numbered in the hundreds of cases or fewer. This likely follows from the older age of many included studies and from their consistent primary focus on clinical data, which is more costly to collect and comes with more restrictions on its use.

## Coherence

The motivational and methodological unity of these studies does not reflect a unified research program. Besides the lack of any primary journal of record, we observed collaborations only among the authors of smaller contiguous programs, including applications of the TWNFI methodology [@Song2006c; @Verma2015a], individualized mortality prediction for ICU patients [@Lee2015; @Lee2017] and the use of more explanatory models to prioritize predictors or treatments for chronic disease [@Ng2015; @Tang2021c; @Ng2021b].

These programs used varying terminology for common concepts, and no common term is in use for what we here refer to as localized modeling; authors described their approaches as "targeted prognosis" [@Mariuzzi1997], "transductive inference" [@Song2006c], "personalized decision support" [@Lee2015], "personalized (predictive) models" [@Liang2015a, in contrast to "local models"; @Ng2015; @Wang2019; @Ma2020a], and "precision cohort" analysis [@Wang2019; @Tang2021c; @Ng2021b]. We find uses of "targeted", "personalized", and "precision" generic and imprecise to this approach, while transductive inference is an established term for a broader set of methods in ML. Because _local_ naturally contrasts with _global_, we propose "localized models" as a suitable term for this counterpart to globally-fitted models.[^local]

[^local]: We note, in response to one reviewer's comment, that for many if not most models used in ML, excepting GLMs, a predictor may play a different and more or less important role for some cases than others, as quantified by importance measures or model-agnostic explanations [@Biecek2021; @Molnar2023]. In this way, such models can also be viewed as "localized". We recognize that the use of global and local explanations is standard. As another reviewer pointed out, the terminology of local and global is also used in the unrelated context of federated ML to distinguish models trained using data stored in a single storage device (local) from their aggregations (global) [@Moshawrab2023; @Brauneck2023]. The prospects for federated architecture to more effectively implement similarity cohort--based models are intriguing, and would require some reconciliation of terms. At present, we feel that these methods are sufficiently disjoint to avoid confusion in practice, but we suggest the more specific term "similarity cohort--based models" when the term "local" is overloaded.

## Synthesis

The 25 studies included in our synthesis used composite techniques that we organized into two major types. One type, similarity learning, was used in 8 studies that tie in to the much larger literature on metric learning [@Yang2006]. These techniques are designed to detect structure in data, especially associations between predictors and responses, and to use this structure to inform the definition of a patient similarity measure. Seven studies used response values in training data sets to supervise similarity learning while two used unsupervised learning (Table \ref{tab:composite}). Based on the key properties of metric learning algorithms identified by @Bellet2014, most learned measures were non-linear while some were local, and some were optimized globally while others locally. However, not all qualified as metric learning, since the resulting measures would not necessarily satisfy the triangle inequality. Each then used its similarity measure to retrieve training cases relevant to each testing case. Once retrieved, most took a standard NN approach to generating predictions from the cohort; two exceptions [@Vilhena2016a; @Tang2021c] proceeded to fit predictive models to the retrieved cohorts and are discussed below.

### Similarity learning and threshold optimization

The similarity learning approaches took a variety of forms. Several studies used unsupervised similarity learning, for example the Mahalanobis distance [@Lowsky2013] and the Kolmogorov entropy-based distance [@Elter2007a]. In particular, @Yearwood1997a defined similarity as a weighted sum of differences in predictor values and used linear regression to optimize the weights for the predictive accuracy of the cohort they retrieve. Others used supervised learning: @Song2006c proposed an iterative algorithm to optimize a set of fuzzy inference rules used to retrieve relevant cases and generate a prediction, using back-propagation on the rules’ parameters. Their method was used in several later studies returned by our search, which are not discussed here because they did not originate the technique. @Nicolas2014a explicitly appeal to a supervised distance metric learning (DML) technique [@Xing2002] to obtain a similarity measure on their set of dermoscopy and confocal images that most effectively separates malignant from benign melanoma tumors. We will discuss the remaining uses of supervised similarity learning shortly.

Distinct from but related to similarity learning was supervised cohort construction. This was an adaptive step taken by @Park2006 to optimize the predictive performance of models fitted to cohorts retrieved using a fixed similarity rather than count threshold, a counterpart to conventional CBR they called "statistical CBR".[^combinatorial] The step used a heuristic procedure to locate a similarity threshold that (locally) maximizes predictive accuracy on the training set. This is analogous to optimizing the neighborhood size parameter in NN predictive modeling, so we will not discuss it further except to mention that the authors found statistical CBR to outperform conventional CBR on several data sets. @CampilloGimenez2012a combined these learning techniques: They used logistic regression on the training set to obtain weights for the predictors (by predicting the binary outcome of kidney transplant waitlist registration) and for the cases (by predicting agreement of outcome between an index case and other training cases), and they used exhaustion to optimize the size of the retrieved cohort for the accuracy of the NN model using the previously optimized weights.

[^combinatorial]: Drawing from @Goyal2008, we suggest the term "combinatorial CBR" for the then-conventional approach using similarity cohorts of fixed cardinality.

As noted above, these instances of similarity learning fit into a much larger literature on metric learning, which has seen widespread use in health informatics and modeling. That these studies satisfied our review criteria is due to the novelty of the techniques at the time of publication and the detailed attention paid to their techniques by the authors. It also reflects a limitation of our study and of the literature we set out to retrieve: We are aware of no standard terms in use to identify the family of techniques that fit statistical models to similarity-based cohorts. The retrieved studies whose use of such techniques alone satisfied our criteria variably termed them individualized, personalized, local, and patient-specific models, among other terms (Table \ref{tab:composite}). We prefer the term localized models, which is both sensitive and specific to uses of a similarity or distance measure to retrieve a cohort to which a model is fit: It includes some properly included techniques we did not have in mind [@Vilhena2016a] yet excludes other techniques commonly referred to as individualized, personalized, or precision.

### Localized models

In addition to @Lowsky2013, @Liang2015a, @Ng2015, @Lee2017, @Vilhena2016a, @Tang2021c, and @Ng2021b, five other studies employed localized models: @Mariuzzi1997, @Lee2015, @Verma2015a, @Wang2019, and @Ma2020a. In most cases these models were purely predictive in application; of the exceptions, @Mariuzzi1997 produced purely descriptive models of survival outcomes, which were evaluated for their precision rather than their accuracy, while @Ng2015 used generalized regression models descriptively as well as predictively, as did some of the same authors [@Tang2021c; @Ng2021b] later. The most common approach to cohort selection was to optimize a size threshold via manual exploration [@Mariuzzi1997; @Lee2015; @Ng2015; @Lee2017; @Wang2019; @Vilhena2016a] or via cross-validation [@Lowsky2013; @Verma2015a]. Further in the former direction, one study [@Ma2020a] set a fixed cohort size while two [@Liang2015a; @Tang2021c; @Ng2021b] devised more sophisticated algorithms to balance multiple desiderata including predictive accuracy.

Several themes emerged from this sample. Foremost was the optimization of retrieved cohort sizes for some measure of model performance. Despite the early recommendation by @Park2006 to threshold cohorts by similarity rather than by cardinality, only the one previously published study @Mariuzzi1997 and the most recent study [@Tang2021c; @Ng2021b] in this corpus took this approach. Viewed as a hyperparameter of the individualized modeling approach, cohort size can be treated in the same way as neighborhood size in NN prediction (viewed here as a special case of localized modeling wherein the model is a simple summary statistic), so we will not discuss it further.

An alternative approach was to optimize multiple hyperparameters together: @Kasabov2010 proposed an iterative algorithm to settle on an optimal number and set of predictors as well as number of training cases (comprising the retrieved cohort) that was later used by @Liang2015a. More recently, @Tang2021c and @Ng2021b proposed a three-step process for cohort selection that filtered by exact match for one subset of predictors, used domain-informed similarity measures to rank these, and optimized the similarity threshold for a trade-off between cohort size and a measure of bias called cohort balance. Notably, though their similarity measure was supervised, this optimization process was not.

With the exception of the descriptive survival models of @Mariuzzi1997, only one thread in this corpus concerned itself with the interpretability of localized models: @Ng2015 fit logistic regression models to similarity-based cohorts and examined not only their predictive performance but the localized sets of largest and most detectable predictors, termed risk profiles, and how they differed across the testing population. This approach informed their later use of localized comparative effectiveness–style models to recommend courses of treatment at decision points during a monitored patent’s stay [@Tang2021c; @Ng2021b]. These studies suggest much wider potential for localized real-world evidence generation, which has long been promoted as a promise of advances in clinical and health informatics [@Longhurst2014].

### A general framework

This relatively small sample exhibited a wide range of approaches, both technically and conceptually. Discussion of the technical diversity of these approaches has been summarized above and published in greater detail in previous reviews [@Choudhury2016; @Sharafoddini2017; @Parimbelli2018], and we focus here on the conceptual. The design of a localized modeling approach can be decomposed into three largely independent choices: (1) how to measure the similarity between cases, (2) how to retrieve a cohort of cases similar to an index case, and (3) how to generate a prediction or other statistical insight for the index case from the similarity cohort. The choices are the _similarity measure_, the _retrieval step_, and the _statistical model_, respectively. These are diagrammed in Figure \ref{fig:framework}.

```{r fig:framework, fig.align='center', out.width="80%", echo=FALSE}
#| fig.cap =
#| "\\label{fig:framework}
#| General framework for localized models."
knitr::include_graphics("../../fig/fig-typology.pdf")
```

Each step can be unsupervised supervised: Unsupervised similarity measures include discrete measures like the Levenshtein distance and continuous measures like cosine similarity, while supervised measures include the Mahalanobis distance and random forest proximity as well as several composite measures with weights calculated from the data. Most retrieval steps obtained cohorts with a uniform size (cardinality) or similarity threshold, and the remaining were likewise only informed by predictors; thus, while a retrieval step could in principle be supervised, in this sample none were. Almost all models were supervised, being that most performed predictive tasks, though @Mariuzzi1997 modeled only survival rates within localized cohorts. Meanwhile, each step can be optimized in a ML fashion by having its parameters tuned to improve performance. We found no studies that tuned the calculation of similarity, though most uniform cohort sizes were tuned. Some earlier studies tuned generalized regression models, but more recent studies have not.

## Directions and expectations

As discussed in Section \ref{sec:rationales}, most studies were premised on the potential predictive value of localized models.
As cautioned in Section \ref{sec:performance}, not all experiments affirmed this premise. Most of those that did involved using metric learning to improve retrieval and sometimes iterative optimization of the metric and of the localized model.
Some studies showcased results using a variety of such specializations [@CampilloGimenez2012a; @Ng2015; @Zhang2018a]. However, because these advanced implementations were mostly compared against their precursors or commonplace alternatives, we cannot speak to their performance or any trade-offs with respect to each other.
This would be a worthy goal of future work, though it might be limited by the lack of open-source code or public implementations (Section \ref{sec:availability}).

While most studies identified outstanding technical needs (Section \ref{sec:needs}), few emphasized the need to assess human-focused qualities like user interface and user experience, interpretability, meaningfulness, or trust.
Most studies also identified interpretability as an advantage of localized models, though none evaluated model interpretability and few proposed new interpretative uses.
Lack of trust in ML tools is a long-recognized problem that direct intepretability of model components could help alleviate, but this is more often assumed than demonstrated.
One valuable direction for future work would be to compare the ability of non-technical investigators or users to draw correct inferences from these and other interpretable methods and their levels of confidence in those inferences.

We posit that an essential component of such "soft" performance goals is the ability of the user community to exert some control over the models.
Given the impact on performance of the choice or optimization of the similarity measure, an important target for user input would be the importance of certain variables in the calculation of similarity, with an understanding of how it can impact not only the performance of the predictions but also the cohort retrieved for the model. For one example, a user may want to minimize the weight of rare diseases in medical history in order to retrieve a population with more such cases in order to better measure their associated risk to the outcome. In contrast, they may want to increase the weight of the indicating diagnosis in order to allow fewer patients from similar but distinct populations to influence the model. For another example, a user may want to down-weight socioeconomic variables like race--ethnicity in order to ensure a more diverse modeling cohort.
Future purely quantitative work could assess whether similarity-tuning can achieve these ends more flexibly than strict inclusion/exclusion criteria.

## Conclusions

We propose the term _localized modeling_ to encompass an approach derived from CBR in which parameterized models are fitted in a standardized way to nearest neighborhoods of past or training cases according to a measure of patient similarity. We conducted a systematic search for studies that apply localized models to tasks involving health data and synthesized these largely independently developed approaches into a general framework. While the search was limited by low inter-rater reliability and failure to recover several motivating examples, the included studies used many of the same underlying tools to build, optimize, and evaluate their methods. We therefore believe that our framework can serve to taxonomize ongoing work of this type and inform the development of customizable implementations. Indeed, the availability of increasing computational power, the diversity of tasks to which these models were applied and of technical specifications they employed, and the apparent lack of any multi-group research program to date suggest great potential for growth. Whereas precious few of the reviewed studies used these models, for any task other than prediction, despite widespread suspicion among clinicians of "black-box" models and growing interest in interpretable alternatives, we recommend that future work put greater emphasis on the development and validation of interpretable localized models and on their reception by communities of medical research and clinical practice.

# Acknowledgments

We are grateful for the astute and supportive comments of three anonymous reviewers.

# Support

The authors received no specific funding or other support for this work.

# Competing interests

The authors declare that they have no competing interests.

# Data and code
\label{sec:data}

Search results are publicly available in a Zotero group library.
Data collected or encoded for included studies are publicly available in two Google Sheets.
Code used to analyze these data and generate this manuscript is publicly available in a GitHub repository.

* Search results, included studies, and other bibliography: <https://www.zotero.org/groups/5017571/imsr/>
* Bibliographic and methodological properties of included studies: <https://docs.google.com/spreadsheets/d/1tpWMhYH2pyRT55K7n2J2XFs-kEV_JTuCDmXzJ4BBgNo/>
* Terminology and composite techniques of included studies: <https://docs.google.com/spreadsheets/d/1xvDJwiLBoI2oz8fxHJ5MjNmiju_RAlK7RJv-wXe1DAs/>
* Code used to conduct analyses and prepare the manuscript: <https://github.com/corybrunson/imsr>

# Appendix

## Research question

How is patient similarity–based individualized modeling conducted using retrospective data?

## Purpose of review
\label{sec:appendix-purpose}

1. Provide a summary of individualized models to date.
2. Lay the groundwork for conducting a comparison study of individualized models.
3. Provide a framework for future individualized modeling studies.

## Search
\label{sec:appendix-search}

The procedure for formulating the search began with an evaluation of the research question. We highlighted specific elements within our topic of interest that we found critical to our search and listed them using an OR of ANDs, pairing terms we believed would provide our desired result. Following the solidification of the search, a thesaurus was created in which each term was expanded by synonyms that are similar enough to our core term to be applicable to our search. The expanded search string was then evaluated using the PubMed Advanced Search platform. We initially included each term and their synonyms separately to evaluate what resulted. Several terms were eliminated due to PubMed classifying them as “phrases not found” and other terms were removed to reduce the number of results, providing a more concise list of results. At the conclusions of this process for each individual term, we combined each search term using an OR of ANDs to ultimately form our search.

The search will have been designed to recover studies of the kind reviewed by the review papers from which we obtained our “seed set”. To validate the final search design, we will determine how many of the papers in this seed set that are indexed by PubMed are actually recovered by our search. In most cases, the focus of a review paper is different from ours, so we will only perform this validation test on the seed set obtained from two review papers that are (a) closest in focus to ours and (b) use terminology associated with the two distinct sub-literatures relevant to our focus: Choudhury & Begum (2016), which focuses on case-based reasoning in medicine, and Sharafoddini, Dubin, & Lee (2017), which focuses on patient similarity–based prediction models on health data. The proportion of each PubMed-indexed seed set that is recovered from our PubMed search provides a rough and optimistic yet useful estimate of the proportion of the relevant literature that our full search strategy will recover.

Once we have finalized the search as a logical pattern, we will take the following steps to generate the sample/corpus of literature that will be the starting point for our selection process.

1. The logical pattern will be converted to a search string using the syntax appropriate to each database in our search strategy. These include PubMed (already done as part of the search design), Web of Science, Academic Search Premier/Elite, and Mathematical Reviews.
2. The search will be conducted on each database and the results organized into a Zotero collection, with one subcollection for each database.
3. Duplicate results will be identified and merged. (A result obtained from multiple databases should have only one Zotero entry but should be filed under the subcollection for each database in which it was found.)

Following discussion among AC, PMJ, and JCB, we discarded results from Google Scholar due to missing abstracts, missing URLs, high overlap with other search results, and irreproducibility of the search process.

### Search strings

Here we reproduce the search strings and platform specifications used in our literature search.
We first finalized the **PubMed** search string below:[^pubmed-url]

[^pubmed-url]: <https://pubmed.ncbi.nlm.nih.gov/?term=(+"case-based+reasoning"+[All+Fields]+OR+"case-based+system"+[All+Fields]+)+OR+(+"individualized+modeling"+[All+Fields]+OR+"personalized+modeling"+[All+Fields]+OR+"customized+modeling"+[All+Fields]+)+OR+(+"individualized+cohort"+[All+Fields]+)+OR+(+(+"patient+similarity"+[All+Fields]+OR+"patient+distance"+[All+Fields]+OR+"patient+connection"+[All+Fields]+OR+"patient+affinity"+[All+Fields]+OR+"patient+clustering"+[All+Fields]+)+AND+(+"cohort+study"+[All+Fields]+)+)&sort=date>

```
(
  "case-based reasoning" [All Fields] OR
  "case-based system" [All Fields]
) OR (
  "individualized modeling" [All Fields] OR
  "personalized modeling" [All Fields] OR
  "customized modeling" [All Fields]
) OR (
  "individualized cohort" [All Fields]
) OR (
  (
    "patient similarity" [All Fields] OR
    "patient distance" [All Fields] OR
    "patient connection" [All Fields] OR
    "patient affinity" [All Fields] OR
    "patient clustering" [All Fields]
  ) AND (
    "cohort study" [All Fields]
  )
)
```

This search yielded 423 results.

We then generated analogous search strings or search strategies for the Web of Science, Academic Search Premier, and MathSciNet platforms, based on the logics and syntaxes of their respective interfaces.

For **Web of Science**, we searched for several separate disjunctions derived from the PubMed search string.
The separate search strings and the number of results obtained using each are below.

------------------------------------------------
Search string                    Number of items
------------------------------- ----------------
`"case-based reasoning" OR`                     
`"case-based system"`                      3,636
                                                
`"individualized model" OR`                  444
`"individualized modeling" OR`                  
`"personalized model" OR`                    
`"personalized modeling" OR`                    
`"customized model"`                         
`"customized modeling"`                         
                                                
`"individualized cohort"`                      1
                                                
`"patient similarity" OR`                     48
`"patient distance" OR`                         
`"patient connection" OR`                       
`"patient affinity" OR`                         
`"patient clustering"`;                         
Refined search: `"cohort"`                      
------------------------------------------------

We found the results of the first search string to be predominantly irrelevant. To reduce review time, these were dropped.
The last search was refined with an additional term following the initial disjunctive search.
Our searches on Web of Science thus yielded 493 results.

When searching **Academic Search Premier**, we checked the option "Scholarly (Peer-Reviewed) Journals", unchecked the option "Apply equivalent subjects", and searched for several separate disjunctions and conjunctions of strings in the "TX All Text" field.
We obtained the resulting citations via email in RIS format.

------------------------------------------------
Search string                    Number of items
------------------------------- ----------------
`"case-based reasoning" OR`                     
`"case-based system"`                      3,283
                                                
`"individualized modeling" OR`               100
`"personalized modeling" OR`                    
`"customized modeling"`                         
                                                
`"individualized cohort"`                      2
                                                
`"patient similarity" AND`                    14
`"cohort study"`                                
                                                
`"patient distance" AND`                      15
`"cohort study"`                                
                                                
`"patient connection" AND`                     6
`"cohort study"`                                
                                                
`"patient affinity" AND`                       0
`"cohort study"`                                
                                                
`"patient clustering" AND`                    50
`"cohort study"`                                
------------------------------------------------

We found the results of the first search string to be predominantly irrelevant. To reduce review time, these were dropped.
Our searches on Academic Search Premier thus yielded 187 results.

For **MathSciNet**, the portal to the _Mathematical Reviews_ database, we used the same separate searches as for Web of Science, in some cases expanded to obtain more results. Those which yielded nonzero numbers of results are below:

------------------------------------------------
Search string                    Number of items
------------------------------- ----------------
`"case-based reasoning"`                     110
                                                
`"individualized model`                        2
                                                
`"patient similarity" AND`                     1
`"cohort"`                                
------------------------------------------------

Our searches of _Mathematical Reviews_ therefore yielded 113 results.

These totaled 1,422 sources from all platforms. We organized the full results in a public Zotero collection alongside the seed set and created a single folder for the 25 results reviewed in detail.

## Screening process
\label{sec:appendix-screening}

**Write this section!**

1. Deduplication (Covidence)
    a. Specific to that source: title, creator, URL, DOI
    b. May overlap: journal, publication date, ISSN
2. Title
    a. Exclude non clinical/non medical setting
    b. Must be in English
    c. Must be original study (not reviews, surveys, opinion, news)
    d. Exclude if search term clearly has different meaning than intended
3. Abstract
4. Full Text
    a. Study criteria (inclusion/exclusion)

## Selection process
\label{sec:appendix-selection}

Our selection of relevant papers from the search corpus will be based on the following inclusion/exclusion criteria:

* Uses labeled case-level (empirical) data set 
* Defines a continuous-valued multivariate case similarity measure
* Uses the similarity measure to select cohorts for index cases from the corpus
* Fits statistical models to cohorts to make inferences about index cases

We decided after concluding full-text review to perform one round of citation-tracking, of citations within the Methods (or analogous) sections of the included entries.

## Analysis and synthesis

Because we could not predict the scope of methodological approaches we would encounter, we did not prepare specific analyses or syntheses _a priori_.

## Performance evaluations and comparisons

\landscapebegin

```{r tab:performance, echo=FALSE, message=FALSE, warning=FALSE}
# TODO: Place vertical separators between major columns.
tab_eval_comp |> 
  pivot_wider(
    id_cols = c(zotero_key, date_of_publication, measure, task),
    names_from = role, values_from = results
  ) |> 
  arrange(date_of_publication) |> 
  mutate(len = map2_int(proposal, comparator, ~ max(length(.x), length(.y)))) |> 
  mutate(across(c(proposal, comparator), ~ map2(., len, \(x, y) x[seq(y)]))) |> 
  unnest(c(proposal, comparator)) |> 
  separate(proposal, c("proposal", " "), sep = ": ") |> 
  separate(comparator, c("comparator", "  "), sep = ": ") |> 
  # use abbreviations throughout
  mutate(across(
    c(proposal, comparator),
    ~ str_replace(., "[Ll]ogistic [Rr]egression", "LR")
  )) |> 
  mutate(across(
    c(proposal, comparator),
    ~ ifelse(. == "Logistic", "LR", .)
  )) |> 
  mutate(across(
    c(proposal, comparator),
    ~ str_replace(., "[Rr]andom [Ff]orest", "RF")
  )) |> 
  mutate(across(
    c(proposal, comparator),
    ~ str_replace(., "[Kk][\\- ][Nn]earest [Nn]eighbor(s){0,1}", "KNN")
  )) |> 
  mutate(across(
    c(proposal, comparator),
    ~ str_replace(., "([Aa]rtificial ){0,1}[Nn]eural [Nn]et(work){0,1}", "NN")
  )) |> 
  mutate(across(
    c(proposal, comparator),
    ~ str_replace(., "[Oo]ne-class", "One-class")
  )) |> 
  mutate(across(
    c(proposal, ` `, comparator, `  `),
    ~ ifelse(is.na(.), "", .)
  )) |> 
  transmute(
    Citation = str_c("[@", zotero_key, "]"),
    Measure = measure, Problem = task,
    Proposals = proposal, ` `,
    # insert blank column to separate right- and left-justified entries
    # `   ` = "",
    Comparators = comparator, `  `
  ) |> 
  mutate(across(
    c(Citation, Measure, Problem),
    ~ ifelse(is.na(lag(.)) | . != lag(.), ., "")
  )) |>
  # prevent numeric list columns from dominating the horizontal space
  # mutate(across(c(Proposals, Comparators), ~ str_c("\\hspace{3em}", .))) |> 
  mutate(across(c(` `, `  `), ~ str_c(., "\\hspace{6em}"))) |> 
  knitr::kable(
    align = "llllrlr",
    caption = paste0(
      "\\label{tab:performance}",
      "Evaluations of proposed methods and comparisons to alternative methods.",
      " (See original studies for full names and descriptions.)"
    )
  )
```

\landscapeend

# References {-}
